from flask import Flask, jsonify, render_template_string, request
import torch
import cv2
import numpy as np
import os
import tempfile
import time
from video_transformer_model import VideoTransformer
from vivit_model import ViViTModel
import base64
from werkzeug.utils import secure_filename
import json
from datetime import datetime

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MB max file size

# C·∫•u h√¨nh
MODEL_PATHS = [
    r"C:\Users\anhvu\Documents\linh tinh\vivit_model.pth",       # PyTorch model file (∆∞u ti√™n ƒë·∫ßu ti√™n)
    r"C:\Users\anhvu\Documents\linh tinh\epoch=4-step=75.ckpt",  # PyTorch Lightning checkpoint  
    "video_transformer_model.ckpt",  # Fallback
]

# Lu√¥n s·ª≠ d·ª•ng CPU v√¨ ƒë√£ ph√°t hi·ªán hi·ªáu su·∫•t t·ªët h∆°n v·ªõi m√¥ h√¨nh nh·ªè
DEVICE = 'cpu'

# Th√¥ng b√°o v·ªÅ vi·ªác ch·ªçn device
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    print(f"ÔøΩ GPU {gpu_name} kh·∫£ d·ª•ng nh∆∞ng kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng (CPU nhanh h∆°n v·ªõi m√¥ h√¨nh nh·ªè)")
print(f"üöÄ ƒêang s·ª≠ d·ª•ng CPU cho video processing ƒë·ªÉ c√≥ hi·ªáu su·∫•t t·ªëi ∆∞u")

ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv', 'flv', 'wmv'}

# Global model instance
model = None
class_names = ['B√¨nh th∆∞·ªùng', 'Gian l·∫≠n']
current_model_path = None

def allowed_file(filename):
    """Ki·ªÉm tra file extension h·ª£p l·ªá"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def load_model():
    """Load model khi kh·ªüi ƒë·ªông app"""
    global model, current_model_path
    
    # T√¨m model file c√≥ s·∫µn
    model_path = None
    print("üîç T√¨m ki·∫øm model files...")
    for path in MODEL_PATHS:
        print(f"   Checking: {path}")
        if os.path.exists(path):
            model_path = path
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"‚úÖ T√¨m th·∫•y model: {os.path.basename(path)} ({size_mb:.1f} MB)")
            break
        else:
            print(f"   ‚ùå Kh√¥ng t·ªìn t·∫°i")
    
    if not model_path:
        print("‚ùå Kh√¥ng t√¨m th·∫•y model file n√†o!")
        print("üí° C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ th·ª≠:")
        for path in MODEL_PATHS:
            print(f"   - {path}")
        return False
    
    try:
        start_time = time.time()
        print(f"üöÄ ƒêang load model t·ª´ {model_path}... ({DEVICE} mode)")
        current_model_path = model_path
        
        # T·ªëi ∆∞u GPU memory n·∫øu ƒëang s·ª≠ d·ª•ng CUDA
        if DEVICE == 'cuda':
            torch.cuda.empty_cache()
            
        if model_path.endswith('.ckpt'):
            # Ki·ªÉm tra lo·∫°i checkpoint
            checkpoint = torch.load(model_path, map_location='cpu')
            state_dict = checkpoint.get('state_dict', checkpoint)
            
            # Ki·ªÉm tra xem c√≥ keys c·ªßa ViViT kh√¥ng
            if any(key.startswith('spatial_transformer') for key in state_dict.keys()):
                print(f"üîß Detected ViViT architecture, loading with ViViT model...")
                model = ViViTModel.load_from_checkpoint_compatible(model_path, device=DEVICE)
            else:
                print(f"üîß Loading standard VideoTransformer checkpoint...")
                try:
                    model = VideoTransformer.load_from_checkpoint(
                        model_path, 
                        map_location='cpu',  # Tr∆∞·ªõc ti√™n load v√†o CPU
                        strict=False
                    )
                except Exception as e1:
                    print(f"‚ö†Ô∏è Lightning load failed: {e1}")
                    print(f"üîÑ Trying manual load...")
                    
                    if 'hyper_parameters' in checkpoint:
                        hparams = checkpoint['hyper_parameters']
                        print(f"üìã Using hyperparameters from checkpoint")
                    else:
                        hparams = {
                            'img_size': 224,
                            'patch_size': 16,
                            'in_channels': 3,
                            'num_classes': 2,
                            'embed_dim': 768,
                            'num_heads': 12,
                            'num_layers': 12,
                            'num_frames': 8,
                            'mlp_ratio': 4.0,
                            'dropout': 0.1,
                            'learning_rate': 1e-4
                        }
                        print(f"üìã Using default hyperparameters")
                    
                    model = VideoTransformer(**hparams)
                    
                    if 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'], strict=False)
                    else:
                        model.load_state_dict(checkpoint, strict=False)
                
        elif model_path.endswith('.pth') or model_path.endswith('.pt'):
            # Load PyTorch pth file - ki·ªÉm tra architecture tr∆∞·ªõc
            print(f"üîß Loading PyTorch .pth file...")
            
            # Ki·ªÉm tra structure c·ªßa pth file
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # Ki·ªÉm tra xem c√≥ keys c·ªßa ViViT kh√¥ng
            if any(key.startswith('spatial_transformer') for key in checkpoint.keys()):
                print(f"üîß Detected ViViT architecture in .pth file, loading with ViViT model...")
                # T·∫°o model ViViT v√† load state dict
                model = ViViTModel()
                model.load_state_dict(checkpoint, strict=False)
            else:
                print(f"üîß Detected standard VideoTransformer architecture in .pth file...")
                model_kwargs = {
                    'img_size': 224,
                    'patch_size': 16,
                    'in_channels': 3,
                    'num_classes': 2,
                    'embed_dim': 768,
                    'num_heads': 12,
                    'num_layers': 12,
                    'num_frames': 8,
                    'mlp_ratio': 4.0,
                    'dropout': 0.1,
                    'learning_rate': 1e-4
                }
                model = VideoTransformer.load_from_pth(model_path, device='cpu', **model_kwargs)
        else:
            print(f"‚ùå Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {model_path}")
            return False
        
        # Di chuy·ªÉn model sang GPU v√† set eval mode
        model.eval()
        
        if DEVICE == 'cuda':
            # Optimized GPU memory usage with mixed precision
            print(f"üîß Moving model to GPU and optimizing with mixed precision...")
            model = model.to(DEVICE)
            
            # Enable automatic mixed precision for faster inference if using PyTorch 1.6+
            if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):
                print(f"‚úÖ Mixed precision inference ƒë∆∞·ª£c b·∫≠t")
        else:
            model = model.to(DEVICE)
        
        # Test forward pass ƒë∆°n gi·∫£n
        print(f"üß™ Testing model forward pass...")
        with torch.no_grad():
            if DEVICE == 'cuda' and hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):
                with torch.cuda.amp.autocast():
                    dummy_input = torch.randn(1, 3, 8, 224, 224).to(DEVICE)
                    try:
                        output = model(dummy_input)
                        print(f"‚úÖ Model forward test successful! Output shape: {output.shape}")
                    except Exception as e:
                        print(f"‚ùå Model forward test failed: {e}")
                        return False
            else:
                dummy_input = torch.randn(1, 3, 8, 224, 224).to(DEVICE)
                try:
                    output = model(dummy_input)
                    print(f"‚úÖ Model forward test successful! Output shape: {output.shape}")
                except Exception as e:
                    print(f"‚ùå Model forward test failed: {e}")
                    return False
        
        load_time = time.time() - start_time
        print(f"‚úÖ Model ƒë√£ load th√†nh c√¥ng tr√™n {DEVICE}! Th·ªùi gian: {load_time:.2f}s")
        print(f"üîß Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Th·ª±c hi·ªán inference warm-up ƒë·ªÉ t·ªëi ∆∞u performance tr√™n GPU
        if DEVICE == 'cuda':
            print(f"üî• Th·ª±c hi·ªán inference warm-up...")
            for _ in range(3):
                with torch.no_grad():
                    if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):
                        with torch.cuda.amp.autocast():
                            _ = model(dummy_input)
                    else:
                        _ = model(dummy_input)
        
        return True
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# HTML template cho web interface
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Fraud Detection</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
            color: #333;
        }
        .upload-section {
            border: 2px dashed #ddd;
            padding: 30px;
            text-align: center;
            border-radius: 10px;
            margin-bottom: 20px;
            transition: border-color 0.3s;
        }
        .upload-section:hover {
            border-color: #007bff;
        }
        .file-input {
            margin: 20px 0;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            width: 100%;
            max-width: 400px;
        }
        .btn {
            background-color: #007bff;
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }
        .btn:hover {
            background-color: #0056b3;
        }
        .btn:disabled {
            background-color: #6c757d;
            cursor: not-allowed;
        }
        .result {
            margin-top: 30px;
            padding: 20px;
            border-radius: 10px;
            display: none;
        }
        .result.normal {
            background-color: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
        }
        .result.fraud {
            background-color: #f8d7da;
            border: 1px solid #f5c6cb;
            color: #721c24;
        }
        .loading {
            display: none;
            text-align: center;
            padding: 20px;
        }
        .spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #007bff;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 20px;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .stats {
            display: flex;
            justify-content: space-around;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .stat-item {
            text-align: center;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 5px;
            margin: 5px;
            flex: 1;
            min-width: 120px;
        }
        .stat-value {
            font-size: 24px;
            font-weight: bold;
            color: #007bff;
        }
        .stat-label {
            font-size: 12px;
            color: #666;
            margin-top: 5px;
        }
        .api-info {
            margin-top: 30px;
            padding: 20px;
            background-color: #e9ecef;
            border-radius: 10px;
        }
        .api-info h3 {
            margin-top: 0;
            color: #495057;
        }
        .code-block {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #007bff;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üé¨ Video Fraud Detection System</h1>
            <p>H·ªá th·ªëng ph√°t hi·ªán h√†nh vi gian l·∫≠n trong video s·ª≠ d·ª•ng VideoTransformer</p>
        </div>

        <div class="upload-section">
            <h3>üì§ Upload Video ƒë·ªÉ Ph√¢n T√≠ch</h3>
            <form id="uploadForm" enctype="multipart/form-data">
                <input type="file" id="videoFile" name="file" class="file-input" accept="video/*" required>
                <br>
                <button type="submit" class="btn" id="analyzeBtn">üîç Ph√¢n T√≠ch Video</button>
            </form>
        </div>

        <div id="loading" class="loading">
            <div class="spinner"></div>
            <p>ƒêang ph√¢n t√≠ch video... Vui l√≤ng ƒë·ª£i</p>
        </div>

        <div id="result" class="result">
            <h3 id="resultTitle"></h3>
            <div class="stats">
                <div class="stat-item">
                    <div class="stat-value" id="prediction">-</div>
                    <div class="stat-label">D·ª± ƒëo√°n</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="confidence">-</div>
                    <div class="stat-label">ƒê·ªô tin c·∫≠y</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="processTime">-</div>
                    <div class="stat-label">Th·ªùi gian x·ª≠ l√Ω</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="fileSize">-</div>
                    <div class="stat-label">K√≠ch th∆∞·ªõc file</div>
                </div>
            </div>
            <div id="details" style="margin-top: 20px;"></div>
        </div>

        <div class="api-info">
            <h3>üîß API Documentation</h3>
            <p>B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng API ƒë·ªÉ t√≠ch h·ª£p v√†o ·ª©ng d·ª•ng c·ªßa m√¨nh:</p>
            
            <h4>Endpoint: POST /predict</h4>
            <div class="code-block">
curl -X POST -F "file=@video.mp4" {{ request.url_root }}predict
            </div>

            <h4>Python Example:</h4>
            <div class="code-block">
import requests

url = "{{ request.url_root }}predict"
files = {"file": open("video.mp4", "rb")}
response = requests.post(url, files=files)
result = response.json()
print(result)
            </div>

            <h4>Response JSON:</h4>
            <div class="code-block">
{
  "success": true,
  "prediction": {
    "class": 0,
    "label": "B√¨nh th∆∞·ªùng",
    "confidence": 0.8542
  },
  "processing_time": 2.345,
  "filename": "video.mp4",
  "file_size": "15.2 MB",
  "timestamp": "2025-01-29T10:30:45"
}
            </div>
        </div>
    </div>

    <script>
        document.getElementById('uploadForm').addEventListener('submit', function(e) {
            e.preventDefault();
            
            const formData = new FormData();
            const fileInput = document.getElementById('videoFile');
            const file = fileInput.files[0];
            
            if (!file) {
                alert('Vui l√≤ng ch·ªçn file video!');
                return;
            }
            
            // Show loading
            document.getElementById('loading').style.display = 'block';
            document.getElementById('result').style.display = 'none';
            document.getElementById('analyzeBtn').disabled = true;
            document.getElementById('analyzeBtn').textContent = '‚è≥ ƒêang x·ª≠ l√Ω...';
            
            formData.append('file', file);
            
            fetch('/predict', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                // Hide loading
                document.getElementById('loading').style.display = 'none';
                document.getElementById('analyzeBtn').disabled = false;
                document.getElementById('analyzeBtn').textContent = 'üîç Ph√¢n T√≠ch Video';
                
                // Show result
                if (data.success) {
                    const prediction = data.prediction;
                    const resultDiv = document.getElementById('result');
                    
                    // Set result class and title
                    resultDiv.className = 'result ' + (prediction.class === 1 ? 'fraud' : 'normal');
                    document.getElementById('resultTitle').textContent = 
                        (prediction.class === 1 ? '‚ö†Ô∏è Ph√°t hi·ªán h√†nh vi gian l·∫≠n!' : '‚úÖ Video b√¨nh th∆∞·ªùng');
                    
                    // Update stats
                    document.getElementById('prediction').textContent = prediction.label;
                    document.getElementById('confidence').textContent = (prediction.confidence * 100).toFixed(1) + '%';
                    document.getElementById('processTime').textContent = data.processing_time.toFixed(2) + 's';
                    document.getElementById('fileSize').textContent = data.file_size;
                    
                    // Show additional details
                    document.getElementById('details').innerHTML = `
                        <p><strong>T√™n file:</strong> ${data.filename}</p>
                        <p><strong>Th·ªùi gian x·ª≠ l√Ω:</strong> ${data.timestamp}</p>
                        <p><strong>M√¥ h√¨nh:</strong> VideoTransformer</p>
                    `;
                    
                    resultDiv.style.display = 'block';
                } else {
                    alert('L·ªói: ' + data.error);
                }
            })
            .catch(error => {
                console.error('Error:', error);
                document.getElementById('loading').style.display = 'none';
                document.getElementById('analyzeBtn').disabled = false;
                document.getElementById('analyzeBtn').textContent = 'üîç Ph√¢n T√≠ch Video';
                alert('C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω video!');
            });
        });
    </script>
</body>
</html>
"""

@app.route('/')
def index():
    """Trang ch·ªß v·ªõi web interface"""
    return render_template_string(HTML_TEMPLATE)

@app.route('/predict', methods=['POST'])
def predict():
    """API endpoint ƒë·ªÉ d·ª± ƒëo√°n video"""
    start_time = time.time()
    
    try:
        # Ki·ªÉm tra model
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng kh·ªüi ƒë·ªông l·∫°i server.'
            }), 500
        
        # Ki·ªÉm tra file upload
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y file trong request'
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c ch·ªçn'
            }), 400
        
        if not allowed_file(file.filename):
            return jsonify({
                'success': False,
                'error': f'File kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Ch·ªâ ch·∫•p nh·∫≠n: {", ".join(ALLOWED_EXTENSIONS)}'
            }), 400
        
        # L∆∞u file t·∫°m th·ªùi
        filename = secure_filename(file.filename)
        temp_dir = tempfile.mkdtemp()
        temp_file_path = os.path.join(temp_dir, filename)
        file.save(temp_file_path)
        
        # L·∫•y th√¥ng tin file
        file_size = os.path.getsize(temp_file_path)
        file_size_mb = file_size / (1024 * 1024)
        
        try:
            # D·ª± ƒëo√°n - b·ªè tham s·ªë DEVICE v√¨ model ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ch·ªâ d√πng CPU
            predicted_class, confidence = model.predict_video(temp_file_path)
            
            if predicted_class is None:
                return jsonify({
                    'success': False,
                    'error': 'Kh√¥ng th·ªÉ x·ª≠ l√Ω video. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng file.'
                }), 400
            
            processing_time = time.time() - start_time
            
            # T·∫°o response
            response = {
                'success': True,
                'prediction': {
                    'class': predicted_class,
                    'label': class_names[predicted_class],
                    'confidence': float(confidence)
                },
                'processing_time': processing_time,
                'filename': filename,
                'file_size': f"{file_size_mb:.1f} MB",
                'timestamp': datetime.now().isoformat(),
                'model_info': {
                    'name': 'VideoTransformer',
                    'device': DEVICE,
                    'classes': class_names
                }
            }
            
            return jsonify(response)
            
        finally:
            # X√≥a file t·∫°m th·ªùi
            try:
                os.remove(temp_file_path)
                os.rmdir(temp_dir)
            except:
                pass
                
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói server: {str(e)}'
        }), 500

@app.route('/health')
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'device': DEVICE,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/model-info')
def model_info():
    """Th√¥ng tin v·ªÅ model"""
    if model is None:
        return jsonify({
            'error': 'Model ch∆∞a ƒë∆∞·ª£c load'
        }), 500
    
    return jsonify({
        'model_name': 'VideoTransformer',
        'classes': class_names,
        'device': DEVICE,
        'parameters': sum(p.numel() for p in model.parameters()),
        'input_format': {
            'video_formats': list(ALLOWED_EXTENSIONS),
            'max_file_size': '100MB',
            'recommended_resolution': '224x224',
            'recommended_fps': '30'
        }
    })

@app.errorhandler(413)
def too_large(e):
    """Handler cho file qu√° l·ªõn"""
    return jsonify({
        'success': False,
        'error': 'File qu√° l·ªõn. K√≠ch th∆∞·ªõc t·ªëi ƒëa l√† 100MB.'
    }), 413

@app.errorhandler(500)
def internal_error(e):
    """Handler cho l·ªói server"""
    return jsonify({
        'success': False,
        'error': 'L·ªói server n·ªôi b·ªô'
    }), 500

if __name__ == '__main__':
    print("üöÄ Kh·ªüi ƒë·ªông Video Fraud Detection API Server...")
    print(f"üì± Device: {DEVICE}")
    print("üéØ T√¨m ki·∫øm model files...")
    
    # Load model
    if not load_model():
        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông server do l·ªói load model")
        exit(1)
    
    print(f"‚úÖ ƒê√£ load model: {os.path.basename(current_model_path)}")
    print("\n‚úÖ Server s·∫µn s√†ng!")
    print("üåê Web interface: http://localhost:5000")
    print("üîß API endpoint: http://localhost:5000/predict")
    print("‚ù§Ô∏è  Health check: http://localhost:5000/health")
    
    # Ch·∫°y Flask app
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=True,
        threaded=True
    )
